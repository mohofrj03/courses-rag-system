import pandas as pd
import numpy as np
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
from tabulate import tabulate
import subprocess
import re

# -----------------------------------
# 1. Load database
# -----------------------------------
df = pd.read_csv("courses_all.csv").fillna("")

documents = (df["title"] + " " + df["description"]).tolist()

# -----------------------------------
# 2. Vectorize
# -----------------------------------
vectorizer = TfidfVectorizer(
    max_features=5000,
    ngram_range=(1, 2)
)
doc_vectors = vectorizer.fit_transform(documents)

# -----------------------------------
# 3. Query
# -----------------------------------
query = input("ğŸ” Ø¹Ø¨Ø§Ø±Øª Ø¬Ø³ØªØ¬Ùˆ Ø±Ø§ ÙˆØ§Ø±Ø¯ Ú©Ù†ÛŒØ¯: ").strip()
query_words = re.findall(r"\w+", query.lower())

query_vector = vectorizer.transform([query])
similarities = cosine_similarity(query_vector, doc_vectors).flatten()

# -----------------------------------
# 4. Filtering (IMPORTANT PART)
# -----------------------------------
SIM_THRESHOLD = 0.15  # Ø¢Ø³ØªØ§Ù†Ù‡ Ø´Ø¨Ø§Ù‡Øª

valid = []

for i, score in enumerate(similarities):
    text = (df.iloc[i]["title"] + " " + df.iloc[i]["description"]).lower()

    # Ø´Ø±Ø· Û±: Ø´Ø¨Ø§Ù‡Øª Ù…Ø¹Ù†Ø§ÛŒÛŒ
    if score < SIM_THRESHOLD:
        continue

    # Ø´Ø±Ø· Û²: Ø­Ø¯Ø§Ù‚Ù„ ÛŒÚ©ÛŒ Ø§Ø² Ú©Ù„Ù…Ø§Øª Ú©ÙˆØ¦Ø±ÛŒ Ø¯Ø§Ø®Ù„ Ù…ØªÙ† Ø¨Ø§Ø´Ø¯
    if not any(word in text for word in query_words):
        continue

    valid.append((i, score))

# Ø§Ú¯Ø± Ù‡ÛŒÚ†ÛŒ Ù†Ø¨ÙˆØ¯
if not valid:
    print("\nâŒ Ù‡ÛŒÚ† Ù†ØªÛŒØ¬Ù‡ Ù…Ø±ØªØ¨Ø·ÛŒ ÛŒØ§ÙØª Ù†Ø´Ø¯.")
    exit()

# -----------------------------------
# 5. Sort & select top
# -----------------------------------
valid = sorted(valid, key=lambda x: x[1], reverse=True)[:5]

results = []
for rank, (idx, score) in enumerate(valid, start=1):
    results.append([
        rank,
        df.iloc[idx]["title"],
        df.iloc[idx]["description"][:150],
        df.iloc[idx]["source"],
        df.iloc[idx]["url"]
    ])

# -----------------------------------
# 6. Show table
# -----------------------------------
headers = ["Ø±ØªØ¨Ù‡", "Ø¹Ù†ÙˆØ§Ù† Ø¯ÙˆØ±Ù‡", "Ø¨Ø®Ø´ÛŒ Ø§Ø² ØªÙˆØ¶ÛŒØ­", "Ù…Ù†Ø¨Ø¹", "Ù„ÛŒÙ†Ú©"]

print("\nğŸ“Š Ù†ØªØ§ÛŒØ¬ Ø¬Ø³ØªØ¬ÙˆÛŒ Ù…Ø¹Ù†Ø§ÛŒÛŒ:\n")
print(tabulate(results, headers=headers, tablefmt="grid"))

# -----------------------------------
# 7. RAG prompt
# -----------------------------------
context = ""
for r in results:
    context += f"- {r[1]} ({r[3]}): {r[2]}\n"

prompt = f"""
ØªÙˆ ÛŒÚ© Ø³ÛŒØ³ØªÙ… RAG Ù‡Ø³ØªÛŒ.
ÙÙ‚Ø· Ø§Ø² Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ø²ÛŒØ± Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ù†.
Ø§Ú¯Ø± Ø¯Ø§Ø¯Ù‡ Ù…Ø±ØªØ¨Ø· ÙˆØ¬ÙˆØ¯ Ù†Ø¯Ø§Ø±Ø¯ØŒ ØµØ±ÛŒØ­ Ø¨Ú¯Ùˆ ÙˆØ¬ÙˆØ¯ Ù†Ø¯Ø§Ø±Ø¯.

Ø§Ø·Ù„Ø§Ø¹Ø§Øª:
{context}

Ø³ÙˆØ§Ù„:
{query}

Ù¾Ø§Ø³Ø®:
"""

result = subprocess.run(
    ["ollama", "run", "qwen2.5:7b"],
    input=prompt.encode("utf-8"),
    stdout=subprocess.PIPE
)

print("\nğŸ§  Ù¾Ø§Ø³Ø® Ù†Ù‡Ø§ÛŒÛŒ Ø³ÛŒØ³ØªÙ…:\n")
print(result.stdout.decode("utf-8", errors="ignore"))
